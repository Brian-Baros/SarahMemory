import React, { useCallback, useEffect, useMemo, useRef, useState } from "react";
import { Canvas, useFrame, useThree } from "@react-three/fiber";
import { OrbitControls, Float, Sparkles, Ring, Torus, Sphere, useGLTF, Html } from "@react-three/drei";
import * as THREE from "three";
import { useSarahStore, AvatarPose } from "@/stores/useSarahStore";
import type { AvatarSpeechCue } from "@/lib/api";

/**
 * AvatarSpec:
 * - This is the “contract” the backend can evolve without breaking the UI.
 * - Start simple: modelUrl + backgroundUrl + pose + expression + speaking/listening.
 * - Later: add blendshapes, motion files, rigs, IK targets, etc.
 */
export type AvatarRenderMode = "procedural_holo" | "gltf_model" | "video_sprite";

export type AvatarSpec = {
  renderMode?: AvatarRenderMode;

  // 3D / media assets (generated by CanvasStudio / VideoEditorCore)
  modelUrl?: string; // GLB/GLTF file URL
  videoUrl?: string; // MP4/WebM for sprite mode (or for “hologram footage”)
  backgroundUrl?: string; // image/video background override
  backgroundType?: "none" | "image" | "video";

  // behavior
  pose?: AvatarPose;
  gesture?: "none" | "wave" | "point" | "nod" | "shake";
  lookAt?: { x: number; y: number; z: number }; // optional target in scene space

  // facial / expression
  expression?: string;
  speaking?: boolean;
  listening?: boolean;

  // lip sync
  speechCues?: AvatarSpeechCue[];
  speechStartTime?: number | null;
};

interface Avatar3DProps {
  speaking?: boolean;
  listening?: boolean;
  expression?: string;

  /**
   * Optional spec coming from backend:
   * - If omitted, we fall back to procedural hologram.
   */
  spec?: AvatarSpec;
}

/* -----------------------------
   Holographic palette
------------------------------ */
const HOLO_PRIMARY = "#00d4ff";
const HOLO_SECONDARY = "#00ffcc";
const HOLO_DARK = "#0a1520";
const HOLO_MID = "#102030";
const HOLO_ACCENT = "#00b8d4";
const HOLO_GLOW = "#40e0d0";

/* -----------------------------
   Small utilities
------------------------------ */
function clamp01(v: number) {
  return Math.max(0, Math.min(1, v));
}

function easeInOut(t: number) {
  return t < 0.5 ? 2 * t * t : 1 - Math.pow(-2 * t + 2, 2) / 2;
}

/* -----------------------------
   Scene background (inside R3F)
   - supports image/video as a plane behind avatar
------------------------------ */
function SceneBackground({ url, type }: { url?: string; type?: "image" | "video" | "none" }) {
  const meshRef = useRef<THREE.Mesh>(null);

  const texture = useMemo(() => {
    if (!url || type === "none") return null;

    if (type === "image") {
      const tex = new THREE.TextureLoader().load(url);
      tex.colorSpace = THREE.SRGBColorSpace;
      return tex;
    }

    if (type === "video") {
      const video = document.createElement("video");
      video.src = url;
      video.muted = true;
      video.loop = true;
      video.playsInline = true;
      video.autoplay = true;
      video.crossOrigin = "anonymous";
      video.play().catch(() => {
        // Autoplay might be blocked until user gesture; it’s okay.
      });
      const tex = new THREE.VideoTexture(video);
      tex.colorSpace = THREE.SRGBColorSpace;
      return tex;
    }

    return null;
  }, [url, type]);

  useFrame(({ camera }) => {
    if (!meshRef.current) return;
    // keep background plane “stuck” behind camera
    const z = -6;
    const dir = new THREE.Vector3();
    camera.getWorldDirection(dir);
    const pos = camera.position.clone().add(dir.multiplyScalar(8));
    meshRef.current.position.copy(pos);
    meshRef.current.quaternion.copy(camera.quaternion);
    meshRef.current.translateZ(z);
  });

  if (!texture) return null;

  return (
    <mesh ref={meshRef}>
      <planeGeometry args={[20, 12]} />
      <meshBasicMaterial map={texture} transparent opacity={0.7} depthWrite={false} />
    </mesh>
  );
}

/* -----------------------------
   Ring FX
------------------------------ */
function HoloRing({ radius, speed, color, yPos = 0 }: { radius: number; speed: number; color: string; yPos?: number }) {
  const ringRef = useRef<THREE.Mesh>(null);

  useFrame((state) => {
    if (ringRef.current) ringRef.current.rotation.z = state.clock.elapsedTime * speed;
  });

  return (
    <Ring ref={ringRef} args={[radius, radius + 0.015, 64]} position={[0, yPos, 0]} rotation={[Math.PI / 2, 0, 0]}>
      <meshBasicMaterial color={color} transparent opacity={0.4} side={THREE.DoubleSide} />
    </Ring>
  );
}

/* -----------------------------
   Procedural Hologram Avatar
   - you can keep your current geometry style
   - but now it’s driven by pose/gesture/spec
------------------------------ */
function ProceduralHoloAvatar({
  expression,
  speaking,
  listening,
  speechCues,
  speechStartTime,
  pose,
  gesture,
  mouseTarget,
}: {
  expression: string;
  speaking: boolean;
  listening: boolean;
  speechCues: AvatarSpeechCue[];
  speechStartTime: number | null;
  pose: AvatarPose;
  gesture?: AvatarSpec["gesture"];
  mouseTarget: { x: number; y: number };
}) {
  const groupRef = useRef<THREE.Group>(null);
  const glowRef = useRef<THREE.Mesh>(null);

  const [mouthOpen, setMouthOpen] = useState(0);
  const [eyeBlink, setEyeBlink] = useState(0);
  const [squint, setSquint] = useState(0);

  // gesture animator (simple front-end placeholder — backend can later send motion clips)
  const gestureRef = useRef<{ t0: number; active: boolean }>({ t0: 0, active: false });

  // expression → color (you can expand)
  const primaryColor = useMemo(() => {
    switch (expression) {
      case "happy":
        return "#00ffcc";
      case "sad":
        return "#00a8ff";
      case "angry":
        return "#00ffaa";
      case "surprised":
        return "#00e8d4";
      default:
        return HOLO_PRIMARY;
    }
  }, [expression]);

  // lip sync
  useEffect(() => {
    if (!speaking) {
      setMouthOpen(0);
      return;
    }

    if (speechCues.length > 0 && speechStartTime) {
      const interval = setInterval(() => {
        const elapsed = Date.now() - speechStartTime;
        let cueValue = 0;

        for (let i = 0; i < speechCues.length; i++) {
          if (speechCues[i].t <= elapsed) {
            cueValue = speechCues[i].v;
          } else {
            if (i > 0) {
              const prev = speechCues[i - 1];
              const next = speechCues[i];
              const t = (elapsed - prev.t) / Math.max(1, next.t - prev.t);
              cueValue = prev.v + (next.v - prev.v) * t;
            }
            break;
          }
        }
        setMouthOpen(clamp01(cueValue));
      }, 33);
      return () => clearInterval(interval);
    }

    // fallback oscillation
    const interval = setInterval(() => {
      const time = Date.now() / 1000;
      const base = Math.sin(time * 8) * 0.25 + 0.3;
      const variation = Math.sin(time * 12) * 0.1;
      setMouthOpen(clamp01(base + variation));
    }, 50);

    return () => clearInterval(interval);
  }, [speaking, speechCues, speechStartTime]);

  // blinking
  useEffect(() => {
    const blink = () => {
      setEyeBlink(1);
      setTimeout(() => setEyeBlink(0), 120 + Math.random() * 60);
    };
    const schedule = () => {
      const delay = 2500 + Math.random() * 3500;
      const timer = setTimeout(() => {
        blink();
        schedule();
      }, delay);
      return timer;
    };
    const timer = schedule();
    return () => clearTimeout(timer);
  }, []);

  // listening squint
  useEffect(() => setSquint(listening ? 0.2 : 0), [listening]);

  // gesture trigger
  useEffect(() => {
    if (gesture === "wave") {
      gestureRef.current = { t0: performance.now(), active: true };
    }
  }, [gesture]);

  useFrame((state) => {
    if (groupRef.current) {
      // float
      groupRef.current.position.y = Math.sin(state.clock.elapsedTime * 0.8) * 0.025;

      // mouse follow
      const targetYaw = mouseTarget.x * 0.3;
      const targetPitch = mouseTarget.y * 0.15;
      groupRef.current.rotation.y = THREE.MathUtils.lerp(groupRef.current.rotation.y, targetYaw, 0.05);

      const basePitch = listening ? 0.06 : 0;
      groupRef.current.rotation.x = THREE.MathUtils.lerp(groupRef.current.rotation.x, basePitch + targetPitch, 0.04);

      // pose offsets (very light placeholders)
      const poseY = pose === "sit" ? -0.15 : 0;
      groupRef.current.position.y += poseY;

      // wave (arm swing placeholder)
      if (gestureRef.current.active) {
        const dt = (performance.now() - gestureRef.current.t0) / 1000;
        const phase = clamp01(dt / 1.2);
        if (phase >= 1) gestureRef.current.active = false;

        // rotate whole avatar slightly as “acknowledgement”
        const a = Math.sin(dt * 10) * 0.04 * (1 - phase);
        groupRef.current.rotation.z = THREE.MathUtils.lerp(groupRef.current.rotation.z, a, 0.2);
      } else {
        groupRef.current.rotation.z = THREE.MathUtils.lerp(groupRef.current.rotation.z, 0, 0.05);
      }
    }

    if (glowRef.current) {
      const material = glowRef.current.material as THREE.MeshBasicMaterial;
      material.opacity = 0.06 + Math.sin(state.clock.elapsedTime * 2) * 0.03;
    }
  });

  return (
    <Float speed={1.2} rotationIntensity={0.08} floatIntensity={0.15}>
      <group ref={groupRef}>
        <Sphere ref={glowRef} args={[1.8, 32, 32]}>
          <meshBasicMaterial color={primaryColor} transparent opacity={0.06} side={THREE.BackSide} />
        </Sphere>

        {/* “Head + body” placeholder: keep your existing procedural mesh if you want.
            This compact block is just a stable base that won’t break when you add GLB mode later. */}
        <group>
          <Sphere args={[0.75, 64, 64]} position={[0, 0.35, 0]}>
            <meshStandardMaterial
              color={HOLO_DARK}
              metalness={0.92}
              roughness={0.1}
              emissive={primaryColor}
              emissiveIntensity={0.03}
            />
          </Sphere>

          {/* Mouth “open” cue (simple) */}
          <Sphere args={[0.12, 16, 16]} position={[0, 0.18 - mouthOpen * 0.03, 0.58]}>
            <meshBasicMaterial color={HOLO_DARK} transparent opacity={0.2 + mouthOpen * 0.5} />
          </Sphere>

          {/* Eye blink glow indicator */}
          <Sphere args={[0.06, 16, 16]} position={[-0.18, 0.43, 0.58]} scale={[1, 1 - eyeBlink * 0.9, 1]}>
            <meshBasicMaterial color={primaryColor} transparent opacity={0.6 * (1 - squint)} />
          </Sphere>
          <Sphere args={[0.06, 16, 16]} position={[0.18, 0.43, 0.58]} scale={[1, 1 - eyeBlink * 0.9, 1]}>
            <meshBasicMaterial color={primaryColor} transparent opacity={0.6 * (1 - squint)} />
          </Sphere>

          {/* Body */}
          <Sphere args={[0.55, 64, 64]} position={[0, -0.55, 0]} scale={[1.05, 1.25, 0.85]}>
            <meshStandardMaterial
              color={HOLO_MID}
              metalness={0.9}
              roughness={0.12}
              emissive={primaryColor}
              emissiveIntensity={0.02}
            />
          </Sphere>
        </group>

        <HoloRing radius={1.05} speed={0.4} color={primaryColor} yPos={0.2} />
        <HoloRing radius={1.2} speed={-0.25} color={HOLO_SECONDARY} yPos={-0.5} />
        <HoloRing radius={0.9} speed={0.55} color={HOLO_ACCENT} yPos={-1.1} />

        <Sparkles count={80} scale={4} size={2} speed={0.4} color={primaryColor} opacity={0.5} />

        {speaking && (
          <Sparkles count={50} scale={2} size={3} speed={2} color={HOLO_GLOW} opacity={0.7} position={[0, -0.2, 0.8]} />
        )}
      </group>
    </Float>
  );
}

/* -----------------------------
   GLTF Avatar (backend-generated)
------------------------------ */
function GltfAvatar({ url, lookAt }: { url: string; lookAt?: { x: number; y: number; z: number } }) {
  const groupRef = useRef<THREE.Group>(null);
  const gltf = useGLTF(url, true);

  useFrame(() => {
    if (!groupRef.current || !lookAt) return;
    // simple look-at (later: drive head bone)
    groupRef.current.lookAt(lookAt.x, lookAt.y, lookAt.z);
  });

  return (
    <group ref={groupRef} position={[0, -1.2, 0]}>
      <primitive object={gltf.scene} />
    </group>
  );
}

/* -----------------------------
   Video “Hologram Sprite”
   - useful while backend generates real rigs/models
------------------------------ */
function VideoSpriteAvatar({ url }: { url: string }) {
  const meshRef = useRef<THREE.Mesh>(null);

  const texture = useMemo(() => {
    const video = document.createElement("video");
    video.src = url;
    video.muted = true;
    video.loop = true;
    video.playsInline = true;
    video.autoplay = true;
    video.crossOrigin = "anonymous";
    video.play().catch(() => {});
    const tex = new THREE.VideoTexture(video);
    tex.colorSpace = THREE.SRGBColorSpace;
    return tex;
  }, [url]);

  useFrame(({ camera }) => {
    if (!meshRef.current) return;
    // billboard toward camera
    meshRef.current.quaternion.copy(camera.quaternion);
  });

  return (
    <mesh ref={meshRef} position={[0, -0.6, 0]}>
      <planeGeometry args={[2.1, 3.2]} />
      <meshBasicMaterial map={texture} transparent opacity={0.95} />
    </mesh>
  );
}

/* -----------------------------
   Camera Controller
------------------------------ */
function CameraController() {
  const { camera } = useThree();
  useEffect(() => {
    camera.position.set(0, 0.3, 2.5);
    camera.lookAt(0, 0.2, 0);
  }, [camera]);
  return null;
}

export function Avatar3D({ speaking = false, listening = false, expression = "neutral", spec }: Avatar3DProps) {
  const { avatarSpeaking, speechCues, speechStartTime, avatarPose, setAvatarPose, triggerWave } = useSarahStore();

  const [mouseTarget, setMouseTarget] = useState({ x: 0, y: 0 });
  const containerRef = useRef<HTMLDivElement>(null);

  // Unified “effective” state (props < store < spec)
  const effective = useMemo(() => {
    const effSpeaking = spec?.speaking ?? (avatarSpeaking || speaking);
    const effListening = spec?.listening ?? listening;
    const effExpression = spec?.expression ?? expression;

    return {
      renderMode: spec?.renderMode ?? "procedural_holo",
      modelUrl: spec?.modelUrl,
      videoUrl: spec?.videoUrl,
      backgroundUrl: spec?.backgroundUrl,
      backgroundType: spec?.backgroundType ?? "none",
      pose: spec?.pose ?? avatarPose,
      gesture: spec?.gesture ?? "none",
      lookAt: spec?.lookAt,
      speaking: effSpeaking,
      listening: effListening,
      expression: effExpression,
      speechCues: spec?.speechCues ?? speechCues,
      speechStartTime: spec?.speechStartTime ?? speechStartTime,
    };
  }, [spec, avatarSpeaking, speaking, listening, expression, speechCues, speechStartTime, avatarPose]);

  // Mouse-follow
  const handleMouseMove = useCallback((e: React.MouseEvent<HTMLDivElement>) => {
    if (!containerRef.current) return;
    const rect = containerRef.current.getBoundingClientRect();
    const x = ((e.clientX - rect.left) / rect.width - 0.5) * 2;
    const y = ((e.clientY - rect.top) / rect.height - 0.5) * -2;
    setMouseTarget({ x, y });
  }, []);

  const handleMouseLeave = useCallback(() => setMouseTarget({ x: 0, y: 0 }), []);

  // Keyboard shortcuts (keep your existing)
  useEffect(() => {
    const onKeyDown = (e: KeyboardEvent) => {
      if (e.target instanceof HTMLInputElement || e.target instanceof HTMLTextAreaElement) return;
      switch (e.key.toLowerCase()) {
        case "w":
          triggerWave();
          break;
        case "s":
          setAvatarPose("sit");
          break;
        case "d":
          setAvatarPose("stand");
          break;
      }
    };
    window.addEventListener("keydown", onKeyDown);
    return () => window.removeEventListener("keydown", onKeyDown);
  }, [setAvatarPose, triggerWave]);

  // If backend requests a pose we don’t have yet, do safe fallback
  const safeRenderMode: AvatarRenderMode = useMemo(() => {
    if (effective.renderMode === "gltf_model" && !effective.modelUrl) return "procedural_holo";
    if (effective.renderMode === "video_sprite" && !effective.videoUrl) return "procedural_holo";
    return effective.renderMode;
  }, [effective.renderMode, effective.modelUrl, effective.videoUrl]);

  return (
    <div ref={containerRef} className="w-full h-full" onMouseMove={handleMouseMove} onMouseLeave={handleMouseLeave}>
      <Canvas
        camera={{ position: [0, 0.3, 2.5], fov: 50 }}
        gl={{ antialias: true, alpha: true }}
        style={{ background: "transparent" }}
      >
        <CameraController />

        {/* Background plane (optional) */}
        <SceneBackground url={effective.backgroundUrl} type={effective.backgroundType} />

        {/* Holo lighting */}
        <ambientLight intensity={0.3} color="#00ffff" />
        <pointLight position={[3, 3, 3]} intensity={0.8} color="#00d4ff" />
        <pointLight position={[-3, 2, 2]} intensity={0.5} color="#00ffcc" />
        <pointLight position={[0, -3, 2]} intensity={0.3} color="#40e0d0" />

        {/* Avatar */}
        {safeRenderMode === "gltf_model" && effective.modelUrl ? (
          <React.Suspense
            fallback={
              <Html center>
                <div className="text-xs text-muted-foreground">Loading avatar model…</div>
              </Html>
            }
          >
            <GltfAvatar url={effective.modelUrl} lookAt={effective.lookAt} />
          </React.Suspense>
        ) : safeRenderMode === "video_sprite" && effective.videoUrl ? (
          <VideoSpriteAvatar url={effective.videoUrl} />
        ) : (
          <ProceduralHoloAvatar
            expression={effective.expression}
            speaking={effective.speaking}
            listening={effective.listening}
            speechCues={effective.speechCues}
            speechStartTime={effective.speechStartTime}
            pose={effective.pose}
            gesture={effective.gesture}
            mouseTarget={mouseTarget}
          />
        )}

        {/* Lock orbit rotation (still allow zoom like your original) */}
        <OrbitControls
          enablePan={false}
          enableZoom={true}
          minDistance={1.2}
          maxDistance={6}
          enableRotate={false}
          target={[0, 0, 0]}
        />
      </Canvas>
    </div>
  );
}

// Allow GLTF caching
useGLTF.preload("/placeholder.glb");
